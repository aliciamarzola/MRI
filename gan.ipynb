{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d12ae67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"✅ Usando dispositivo:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a93654",
   "metadata": {},
   "source": [
    "## Carregar e tratar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd092c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78, 180, 216, 180) (78, 180, 216, 180)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('dataset_t1c_to_t2w.npz')\n",
    "X = data['X']\n",
    "Y = data['Y']\n",
    "\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abae8322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar dimensão de canal\n",
    "X = X[..., np.newaxis]\n",
    "Y = Y[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dac872a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar\n",
    "X = X / np.max(X)\n",
    "Y = Y / np.max(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "062aef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.Y = Y.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Transpor para CxDxHxW (PyTorch espera canal primeiro)\n",
    "        x = torch.tensor(self.X[idx]).permute(3,0,1,2)\n",
    "        y = torch.tensor(self.Y[idx]).permute(3,0,1,2)\n",
    "        return x, y\n",
    "\n",
    "train_dataset = MRIDataset(X, Y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7515e003",
   "metadata": {},
   "source": [
    "### Processar teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d4446e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Encontrados 188 pares T1C-T2F em datasets/brats/validation_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MRIDatasetFolder(Dataset):\n",
    "    def __init__(self, folder_path):\n",
    "        self.folder_path = folder_path\n",
    "        self.pairs = []\n",
    "\n",
    "        # Procurar arquivos T1C e T2F (ou T2W) em todas as subpastas\n",
    "        t1_files = sorted(glob.glob(os.path.join(folder_path, \"**/*t1c.nii.gz\"), recursive=True))\n",
    "        t2_files = sorted(glob.glob(os.path.join(folder_path, \"**/*t2f.nii.gz\"), recursive=True))\n",
    "\n",
    "        # Fazer correspondência por paciente\n",
    "        for t1_path in t1_files:\n",
    "            patient_id = os.path.basename(t1_path).split(\"-t1c\")[0]\n",
    "            match = [t2 for t2 in t2_files if patient_id in t2]\n",
    "            if match:\n",
    "                self.pairs.append((t1_path, match[0]))\n",
    "\n",
    "        print(f\"✅ Encontrados {len(self.pairs)} pares T1C-T2F em {folder_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t1_path, t2_path = self.pairs[idx]\n",
    "\n",
    "        x = nib.load(t1_path).get_fdata().astype(np.float32)\n",
    "        y = nib.load(t2_path).get_fdata().astype(np.float32)\n",
    "\n",
    "        # Normalização simples\n",
    "        x = x / np.max(x) if np.max(x) > 0 else x\n",
    "        y = y / np.max(y) if np.max(y) > 0 else y\n",
    "\n",
    "        factors = [n / s for n, s in zip((180, 216, 180), x.shape)]\n",
    "        x = zoom(x, factors, order=1)\n",
    "        factors = [n / s for n, s in zip((180, 216, 180), y.shape)]\n",
    "        y = zoom(y, factors, order=1)\n",
    "\n",
    "        # Adicionar canal\n",
    "        x = torch.tensor(x[np.newaxis, ...])\n",
    "        y = torch.tensor(y[np.newaxis, ...])\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "val_dataset = MRIDatasetFolder(\"datasets/brats/validation_data\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50618506",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a1e7f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerador: U-Net 3D \n",
    "class UNet3DGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, features=32):\n",
    "        super(UNet3DGenerator, self).__init__()\n",
    "        self.encoder1 = self.contract_block(in_channels, features)\n",
    "        self.encoder2 = self.contract_block(features, features*2)\n",
    "        self.encoder3 = self.contract_block(features*2, features*4)\n",
    "        self.encoder4 = self.contract_block(features*4, features*8)\n",
    "\n",
    "        self.middle = self.contract_block(features*8, features*16)\n",
    "\n",
    "        self.up4 = self.expand_block(features*16, features*8)\n",
    "        self.up3 = self.expand_block(features*16, features*4)\n",
    "        self.up2 = self.expand_block(features*8, features*2)\n",
    "        self.up1 = self.expand_block(features*4, features)\n",
    "\n",
    "        self.final = nn.Conv3d(features*2, out_channels, kernel_size=1)\n",
    "\n",
    "    def contract_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        return block\n",
    "\n",
    "    def expand_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        return block\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "        e4 = self.encoder4(e3)\n",
    "\n",
    "        mid = self.middle(e4)\n",
    "\n",
    "        d4 = self.up4(mid)\n",
    "        d4 = torch.cat((d4, e4), dim=1)\n",
    "        d3 = self.up3(d4)\n",
    "        d3 = torch.cat((d3, e3), dim=1)\n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat((d2, e2), dim=1)\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat((d1, e1), dim=1)\n",
    "\n",
    "        out = torch.tanh(self.final(d1))\n",
    "        return out\n",
    "\n",
    "\n",
    "#  Discriminador: PatchGAN 3D\n",
    "class PatchGANDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=2, features=32):\n",
    "        super(PatchGANDiscriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, features, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv3d(features, features*2, 4, 2, 1),\n",
    "            nn.BatchNorm3d(features*2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv3d(features*2, features*4, 4, 2, 1),\n",
    "            nn.BatchNorm3d(features*4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv3d(features*4, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # concatena T1 (entrada) e T2 (real ou fake)\n",
    "        input = torch.cat((x, y), dim=1)\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9355cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções de perda\n",
    "\n",
    "criterion_GAN = nn.BCEWithLogitsLoss()\n",
    "criterion_L1 = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13c16fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pix2pix(train_loader, num_epochs=5, lr=2e-4, lambda_L1=100):\n",
    "    G = UNet3DGenerator().to(device)\n",
    "    D = PatchGANDiscriminator().to(device)\n",
    "\n",
    "    optimizer_G = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        for t1, t2 in loop:\n",
    "            t1, t2 = t1.to(device), t2.to(device)\n",
    "\n",
    "\n",
    "            # Treina o Discriminador\n",
    "\n",
    "            fake_t2 = G(t1)\n",
    "            D_real = D(t1, t2)\n",
    "            D_fake = D(t1, fake_t2.detach())\n",
    "\n",
    "            real_labels = torch.ones_like(D_real)\n",
    "            fake_labels = torch.zeros_like(D_fake)\n",
    "\n",
    "            loss_D_real = criterion_GAN(D_real, real_labels)\n",
    "            loss_D_fake = criterion_GAN(D_fake, fake_labels)\n",
    "            loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "\n",
    "            # Treina o Gerador\n",
    "\n",
    "            D_fake_for_G = D(t1, fake_t2)\n",
    "            loss_G_GAN = criterion_GAN(D_fake_for_G, real_labels)\n",
    "            loss_G_L1 = criterion_L1(fake_t2, t2) * lambda_L1\n",
    "            loss_G = loss_G_GAN + loss_G_L1\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            loop.set_postfix({\n",
    "                \"Loss_D\": loss_D.item(),\n",
    "                \"Loss_G\": loss_G.item(),\n",
    "                \"L1\": loss_G_L1.item()\n",
    "            })\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab4817",
   "metadata": {},
   "source": [
    "## Treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65489fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]:   0%|          | 0/39 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 10 but got size 11 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m G = \u001b[43mtrain_pix2pix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_L1\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtrain_pix2pix\u001b[39m\u001b[34m(train_loader, num_epochs, lr, lambda_L1)\u001b[39m\n\u001b[32m     11\u001b[39m t1, t2 = t1.to(device), t2.to(device)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Treina o Discriminador\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m fake_t2 = \u001b[43mG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m D_real = D(t1, t2)\n\u001b[32m     18\u001b[39m D_fake = D(t1, fake_t2.detach())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alicia\\VSCode\\MRI\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alicia\\VSCode\\MRI\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mUNet3DGenerator.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     41\u001b[39m mid = \u001b[38;5;28mself\u001b[39m.middle(e4)\n\u001b[32m     43\u001b[39m d4 = \u001b[38;5;28mself\u001b[39m.up4(mid)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m d4 = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43md4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me4\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m d3 = \u001b[38;5;28mself\u001b[39m.up3(d4)\n\u001b[32m     46\u001b[39m d3 = torch.cat((d3, e3), dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Sizes of tensors must match except in dimension 1. Expected size 10 but got size 11 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "G = train_pix2pix(train_loader, num_epochs=100, lr=2e-4, lambda_L1=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b23d141",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(G.state_dict(), \"pix2pix3d_t1_to_t2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96c2341",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_example(model, dataset):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        t1, t2 = dataset[0]\n",
    "        t1 = t1.unsqueeze(0).to(device)\n",
    "        fake_t2 = model(t1).cpu().squeeze().numpy()\n",
    "        real_t2 = t2.squeeze().numpy()\n",
    "        t1_np = t1.cpu().squeeze().numpy()\n",
    "\n",
    "        # Visualiza uma fatia axial central\n",
    "        mid = fake_t2.shape[0] // 2\n",
    "        plt.figure(figsize=(12,4))\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(t1_np[mid,:,:], cmap='gray')\n",
    "        plt.title(\"Entrada T1\")\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(fake_t2[mid,:,:], cmap='gray')\n",
    "        plt.title(\"T2 Sintético (Gerado)\")\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(real_t2[mid,:,:], cmap='gray')\n",
    "        plt.title(\"T2 Real (Ground Truth)\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc2b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_example(G, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3523f5",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f455b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = UNet3DGenerator().to(device)\n",
    "G.load_state_dict(torch.load(\"pix2pix3d_t1_to_t2.pth\"))\n",
    "G.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from evaluation import evaluate_model\n",
    "\n",
    "results = evaluate_model(G, val_loader)\n",
    "print(\"Resultados de avaliação:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
